<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="NeurIPS 2023: Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow">
  <meta property="og:title" content="Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow"/>
  <meta property="og:description" content="Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings."/>
  <meta property="og:url" content="https://sizhewei.github.io/projects/cobevflow"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="./static/images/banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow">
  <meta name="twitter:description" content="NeurIPS 2023: Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="./static/images/banner.png">
  <meta name="twitter:card" content="NeurIPS 2023: Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="NeurIPS 2023; CoBEVFlow; asynchronous co-perception; collaborative perception; bev flow">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow</title>
  <link rel="icon" type="image/x-icon" href="./static/images/logo/sjtu.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sizhewei.github.io/" target="_blank">Sizhe Wei</a><sup> 1&dagger;</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.cn/incareer/in/ACoAADSxRKcB7zJIIKFvPU9bvO1G2BT7Mx6S4vw" target="_blank">Yuxi Wei</a><sup> 1&dagger;</sup>,
              </span>
              <span class="author-block">
                <a href="https://phyllish.github.io" target="_blank">Yue Hu</a><sup> 1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yifanlu0227.github.io/" target="_blank">Yifan Lu</a><sup> 1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Bv8l8jkAAAAJ&hl=en&authuser=1" target="_blank">Yiqi Zhong</a><sup> 2</sup>,
              </span>
              <span class="author-block">
                <a href="http://siheng-chen.github.io/" target="_blank">Siheng Chen</a><sup> 1,3*</sup>,
              </span>
              <span class="author-block">
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/" target="_blank">Ya Zhang</a><sup> 1,3*</sup>
              </span>

            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- <br> -->
                    <span class="author-block">
                      <a href="https://sjtu.edu.cn/CN/Default.aspx" target="_blank"><img src="./static/images/logo/sjtu-full.png" alt="CMIC Logo" style="height:80px;"></a><br><sup>1 </sup><a href="https://cmic.sjtu.edu.cn/CN/Default.aspx" target="_blank">CMIC</a>, <a href="https://en.sjtu.edu.cn" target="_blank">Shanghai Jiao Tong University</a>
                  </span>
                  <span class="author-block">
                    <a href="https://www.usc.edu" target="_blank"><img src="./static/images/logo/usc-full.jpeg" alt="USC Logo" style="height:80px;"></a><br><sup>2 </sup><a href="https://www.usc.edu" target="_blank">University of Southern California</a>
                  </span>
                  <span class="author-block">
                    <a href="https://www.shlab.org.cn" target="_blank"><img src="./static/images/logo/ailab.png" alt="Shanghai AI Lab Logo" style="height:80px;"></a><br><sup>3 </sup><a href="https://www.shlab.org.cn" target="_blank">Shanghai AI Lab</a>
                  </span>
                  
                    <!-- <span class="author-block">Institution Name -->
                    <span class="eql-cntrb"><small><br><sup>&dagger;</sup> Equal Contribution &nbsp;&nbsp;<sup>*</sup> Corresponding Author</small></span>

                    <br><span class="author-block">NeurIPS 2023</span> 
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/2309.16940" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->
                      <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.16940" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MediaBrain-SJTU/CoBEVFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                <!-- Hugging Face Link TODO: -->
                <!-- <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <table align="center">
        <tbody><tr>
          <td align="center" width="50%">
            <p style="font-size: 120%;">Asynchronous Co-Perception 😩 </p>
            <p><img class="center"  src="./static/images/cobevflow_viz/viz_wo_cobevflow/0153_croped_750_90_2112_710.gif" width="90%"></p>
            <p><img class="left"  src="./static/images/cobevflow_viz/viz_wo_cobevflow/0183_croped_750_90_2112_710.gif" width="90%"></p>
            <p><img class="left"  src="./static/images/cobevflow_viz/viz_wo_cobevflow/0390_croped_800_90_2162_710.gif" width="90%"></p>
          </td>
          <td align="center" width="50%">
            <p style="font-size: 120%;">With our CoBEVFlow 🤩 </p>
            <p><img class="center"  src="./static/images/cobevflow_viz/viz_w_cobevflow/0153_croped_750_90_2112_710.gif" width="90%"></p>
            <p><img class="center"  src="./static/images/cobevflow_viz/viz_w_cobevflow/0183_croped_750_90_2112_710.gif" width="90%"></p>
            <p><img class="center"  src="./static/images/cobevflow_viz/viz_w_cobevflow/0390_croped_800_90_2162_710.gif" width="90%"></p>
          </td>
        </tr></tbody>
      </table>
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <font color="red">Red</font> and <font color="green">green</font> boxes denote detection results and ground-truth respectively.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings. The code is available at <a href="https://github.com/MediaBrain-SJTU/CoBEVFlow">https://github.com/MediaBrain-SJTU/CoBEVFlow</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3"><center>Architecture</center></h2>
          <div class="level-set has-text-justified">
            <p>
              The problem of asynchrony results in the misplacements of moving objects in the collaboration messages. That is, the collaboration messages from multiple agents would record various positions for the same moving object. The proposed CoBEVFlow addresses this issue with two key ideas: i) we use a BEV flow map to capture the motion in a scene, enabling motion-guided reassigning asynchronous perceptual features to appropriate positions; and ii) we generate the region of interest(ROI) to make sure that the reassignment only happens to the areas that potentially contain objects. By following these two ideas, we eliminate direct modification of the features and keep the background feature unaltered, effectively avoiding unnecessary noise in the learned features. Figure 1 is the overview of the CoBEVFlow. More tech details can be found in our paper. 
            </p>
          </div>
          <br><img src="./static/images/method.jpg" alt="CoBEVFlow Framework" class="center-image">
          <p style="font-size: 90%;color: #999999">
            Figure 1. Message packing process prepares ROI and sparse features as the message for efficient communication and BEV flow map generation. Message fusion process generates and applies BEV flow map for compensation, and fuses the features at the current timestamp from all agents.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-light">
  
    <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="columns is-centered"> -->
      <!-- <div class="column is-full"> -->
        <div class="hero-body">
          <div class="container">
            <center>  
            <div class="column is-four-fifths">
              <h2 class="title is-3">Experiment Results</h2>
              <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="./static/images/main-results.png" alt="Main results." class="center-image">
                <h2 class="subtitle has-text-centered"><p><left>Figure 2. Comparison of the performance of CoBEVFlow and other baseline methods under the expectation of time interval from 0 to 500ms. CoBEVFlow outperforms all the baseline methods and shows great robustness under any level of asynchrony on both two datasets.</left></p></h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="./static/images/result-volume.png" alt="Comm-Cost result."/>
                <h2 class="subtitle has-text-centered">
                  Figure 3. Trade-off between detection performance (AP@0.50/0.70) and communication bandwidth under asynchrony (expected 300ms latency) on IRV2V dataset. CoBEVFlow outperforms even with a much smaller communication volume.
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="./static/images/viz-1.png" alt="Visulization on IRV2V."/>
                <h2 class="subtitle has-text-centered">
                  Figure 4. Visualization of detection results for V2X-ViT, SyncNet, and CoBEVFlow with the expectation of time intervals are 100, 300, and 500ms on IRV2V dataset. CoBEVFlow qualitatively outperforms the others under different asynchrony. <font color="red">Red</font> and <font color="green">green</font> boxes denote detection results and ground-truth respectively.
              </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="./static/images/viz-2.png" alt="Visulization on DAIR-V2X."/>
                <h2 class="subtitle has-text-centered">
                  Figure 5. Visualization of detection results for Where2comm, V2X-ViT, SyncNet, and CoBEVFlow with the expectation of time intervals are 100, 300, and 500ms on DAIR-V2X dataset. CoBEVFlow qualitatively outperforms the others under different asynchrony. <font color="red">Red</font> and <font color="green">green</font> boxes denote detection results and ground-truth respectively.
                </h2>
              </div>
              </div>
            </div>
            </center>
          </div>
        </div>
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
          <div class="level-set has-text-justified">
            <h4 class="title is-4"><b> Benchmark Comparison</b></h4>
      <p><left>
        The baseline methods include late fusion, DiscoNet, V2VNet, V2X-ViT and Where2comm. The red dashed line represents single-agent detection without collaboration. We also consider the integration of SyncNet with Where2comm, which presents the SOTA method Where2comm with resistance to time delay. All methods use the same feature encoder based on PointPillars. To simulate temporal asynchrony, we sample the frame intervals of received messages with binomial distribution to get random irregular time intervals. Fig. 4 shows the detection performances (AP@IoU=0.50/0.70) of the proposed CoBEVFlow and the baseline methods under varying levels of temporal asynchrony on both IRV2V and DAIR-V2X, where the x-axis is the expectation of the time interval of delay of the latest received information and interval between adjacent frames and y-axis the detection performance. Note that, when the x-axis is at 0, it represents standard collaborative perception without any asynchrony. We see that i) the proposed CoBEVFlow achieves the best performance in both simulation and real-world datasets at all asynchronous settings. On the IRV2V dataset, CoBEVFlow outperforms the best methods by 23.3% and 35.3% in terms of AP@0.50 and AP@0.70, respectively, under a 300ms interval expectation. Similarly, under a 500ms interval expectation, we achieve 30.3% and 28.2% improvements, respectively. On DAIR-V2X dataset, CoBEVFlow still performs best. ii) CoBEVFlow demonstrates remarkable robustness to asynchrony. As shown by the red line in the graph, CoBEVFlow exhibits a decrease of only 4.94% and 14.0% in AP@0.50 and AP@0.70, respectively, on the IRV2V dataset under different asynchrony. These results far exceed the performance of single-object detection, even under extreme asynchrony.
      </left></p>

          <img src="./static/images/main-results.png" alt="Main results." class="center-image">
          <p style="font-size: 90%;color: #999999">
            Figure 2. Comparison of the performance of CoBEVFlow and other baseline methods under the expectation of time interval from 0 to 500ms. CoBEVFlow outperforms all the baseline methods and shows great robustness under any level of asynchrony on both two datasets.
          </p>
          <br>
          <h4 class="title is-4"><b> Communication Cost </b></h4>
          <p><left>
            CoBEVFlow allows agents to share only sparse perceptual features and the ROI set, which is communication bandwidth friendly. Figure 3 compares the proposed CoBEVFlow with the previous methods in terms of the trade-off between detection performance (AP@0.50/0.70)  and communication bandwidth under asynchrony. We adopt the same asynchrony settings mentioned before and choose 300ms as the expectation of the time interval. We see: i) CoBEVFlow consistently outperforms the state-of-the-art communication efficient solution, where2comm, as well as the other baselines in the setting of asynchrony; ii) as the communication volume increases, the performance of CoBEVFlow continues to improve steadily, while the performance of where2comm and where2comm+SyncNet fluctuates due to improper information transformation caused by asynchrony.
          </left></p>
          <img src="./static/images/result-volume.png" alt="Comm-Cost result." class="center-image">
          <p style="font-size: 90%;color: #999999">
            
          </p>

          <h4 class="title is-4"><b> </b></h4>
          <p><left>
            
          </left></p>
          <img src="./static/ " alt=" " class="center-image">
          <p style="font-size: 90%;color: #999999">
            
          </p>

          <h4 class="title is-4"><b> </b></h4>
          <p><left>
            
          </left></p>
          <img src="./static/ " alt=" " class="center-image">
          <p style="font-size: 90%;color: #999999">
            
          </p>

          <h4 class="title is-4"><b> </b></h4>
          <p><left>
            
          </left></p>
          <img src="./static/ " alt=" " class="center-image">
          <p style="font-size: 90%;color: #999999">
            
          </p>

        </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
    @inproceedings{wei2023asynchronyrobust,
      title={Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow}, 
      author={Sizhe Wei and Yuxi Wei and Yue Hu and Yifan Lu and Yiqi Zhong and Siheng Chen and Ya Zhang},
      booktitle = {Advances in Neural Information Processing Systems},
      year={2023}
    }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" align="right">
          <p>
            Project Template: <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
